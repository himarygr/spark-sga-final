{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Obtaining dependency information for py4j==0.10.9.7 from https://files.pythonhosted.org/packages/10/30/a58b32568f1623aaad7db22aa9eafc4c6c194b429ff35bdc55ca2726da47/py4j-0.10.9.7-py2.py3-none-any.whl.metadata\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840633 sha256=cd47b2ac374164f45bd6cf4a2a7eafb736669a2830c1cc4021bdfc9e57281093\n",
      "  Stored in directory: /Users/mac/Library/Caches/pip/wheels/97/f5/c0/947e2c0942b361ffe58651f36bd7f13772675b3863fd63d1b1\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.functions import split, col, lower, row_number, collect_list, struct, sort_array, expr, concat_ws\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 02:50:51 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session in local mode\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ClickstreamRouteAnalysis\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"delimiter\", \"\\t\").csv(\"clickstream.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584160|\n",
      "|    562|       507|        page|    rabota|1695584166|\n",
      "|    562|       507|       event|    rabota|1695584174|\n",
      "|    562|       507|       event|    rabota|1695584181|\n",
      "|    562|       507|       event|    rabota|1695584189|\n",
      "|    562|       507|        page|      main|1695584194|\n",
      "|    562|       507|       event|      main|1695584204|\n",
      "|    562|       507|       event|      main|1695584211|\n",
      "|    562|       507|       event|      main|1695584211|\n",
      "|    562|       507|       event|      main|1695584219|\n",
      "|    562|       507|        page|     bonus|1695584221|\n",
      "|    562|       507|        page|    online|1695584222|\n",
      "|    562|       507|       event|    online|1695584230|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify error events by checking if the event_type contains 'error' (case insensitive), flagging them as 1 in the 'error_flag' column\n",
    "df = df.withColumn(\"error_flag\", F.when(F.lower(F.col(\"event_type\")).contains(\"error\"), 1).otherwise(0))\n",
    "\n",
    "# Define a window that partitions by user and session, ordering by timestamp to calculate a cumulative sum of error flags within each session\n",
    "window_spec = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df = df.withColumn(\"cumulative_error\", F.sum(\"error_flag\").over(window_spec))\n",
    "\n",
    "# Filter out rows after the first error in each session, keeping only those where no cumulative error has occurred, then drop error columns\n",
    "filtered_df = df.filter(F.col(\"cumulative_error\") == 0).drop(\"error_flag\", \"cumulative_error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a session window to analyze consecutive events and remove duplicates within each session based on the event page\n",
    "window_session = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\")\n",
    "filtered_df = (\n",
    "    filtered_df.withColumn(\"prev_page\", F.lag(\"event_page\").over(window_session))  # Check previous event page\n",
    "               .filter((F.col(\"event_page\") != F.col(\"prev_page\")) | F.col(\"prev_page\").isNull())  # Keep if page is different from previous or if it's the first event\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the event pages to form a route, then count and rank the most frequent routes, limiting to the top 30 routes\n",
    "routes_df = (\n",
    "    filtered_df.groupBy(\"user_id\", \"session_id\")\n",
    "               .agg(F.collect_list(\"event_page\").alias(\"pages\"))  # Collect event pages as a list\n",
    "               .withColumn(\"route\", F.expr(\"array_join(pages, '-')\"))  # Join the list to form a single route\n",
    "               .groupBy(\"route\")\n",
    "               .count()\n",
    "               .orderBy(F.desc(\"count\"))\n",
    "               .limit(30)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_10_routes_df = routes_df.limit(10)\n",
    "\n",
    "top_10_routes_dict = {row[\"route\"]: row[\"count\"] for row in top_10_routes_df.collect()}\n",
    "\n",
    "with open(\"result.json\", \"w\") as file:\n",
    "    json.dump(top_10_routes_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8999999999999999\n",
      "Correct main answer!\n",
      "Correct main-archive answer!\n",
      "Correct main-rabota answer!\n",
      "Correct main-internet answer!\n",
      "Correct main-bonus answer!\n",
      "Correct main-news answer!\n",
      "Correct main-tariffs answer!\n",
      "Correct main-online answer!\n",
      "Wrong main-vklad answer!\n",
      "Correct main-rabota-archive answer!\n"
     ]
    }
   ],
   "source": [
    "!curl -F file=@result.json 51.250.123.136:80/MDS-LSML1/mlagosha/w6/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:=============================>                            (1 + 1) / 2]\r"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to RDD for further processing\n",
    "data_rdd = df.rdd.map(tuple)  # Convert DataFrame rows to tuples\n",
    "\n",
    "# Identify sessions containing error events and extract the timestamp of each error\n",
    "session_errors = data_rdd.filter(lambda x: \"error\" in x[2].lower()).map(lambda x: ((x[0], x[1]), x[4]))\n",
    "\n",
    "# Get the earliest error timestamp within each session\n",
    "first_error_ts = session_errors.reduceByKey(lambda a, b: min(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter events that occur before the first error in each session\n",
    "data_rdd = data_rdd.map(lambda x: ((x[0], x[1]), x)) \\\n",
    "                   .leftOuterJoin(first_error_ts) \\\n",
    "                   .filter(lambda x: x[1][1] is None or x[1][0][4] < x[1][1]) \\\n",
    "                   .map(lambda x: x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sort events by user, session, and timestamp to maintain sequence\n",
    "sorted_rdd = data_rdd.sortBy(lambda x: (x[0], x[1], x[4]))\n",
    "\n",
    "def deduplicate_consecutive_pages(records):\n",
    "    previous_page = None\n",
    "    for record in records:\n",
    "        if record[3] != previous_page:  \n",
    "            yield record[3]\n",
    "        previous_page = record[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main\t8184\n",
      "main-archive\t1113\n",
      "main-rabota\t1047\n",
      "main-internet\t897\n",
      "main-bonus\t870\n",
      "main-news\t769\n",
      "main-tariffs\t677\n",
      "main-online\t587\n",
      "main-vklad\t518\n",
      "main-rabota-archive\t170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Group by user_id and session_id, deduplicate consecutive pages, and filter sessions with at least one page\n",
    "session_routes = sorted_rdd.map(lambda x: ((x[0], x[1]), x)).groupByKey() \\\n",
    "                           .mapValues(deduplicate_consecutive_pages) \\\n",
    "                           .mapValues(lambda pages: list(pages)) \\\n",
    "                           .filter(lambda x: len(x[1]) >= 1)\n",
    "\n",
    "\n",
    "route_counts = session_routes.map(lambda x: (\"-\".join(x[1]), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "top_10_routes = route_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "for route, count in top_10_routes:\n",
    "    print(f\"{route}\\t{count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark error events by adding a flag column, making it 1 where event_type contains 'error'\n",
    "df = df.withColumn(\"error_flag\", F.when(F.lower(F.col(\"event_type\")).like(\"%error%\"), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, session_id: string, event_type: string, event_page: string, timestamp: string, error_flag: int, cumulative_error: bigint, error_accumulated: bigint]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_window = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df = df.withColumn(\"error_accumulated\", F.sum(\"error_flag\").over(error_window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.filter(F.col(\"error_accumulated\") == 0).drop(\"error_flag\", \"error_accumulated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, session_id: string, event_type: string, event_page: string, timestamp: string, cumulative_error: bigint]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_window = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\")\n",
    "filtered_df = (\n",
    "    filtered_df.withColumn(\"previous_page\", F.lag(\"event_page\").over(session_window))\n",
    "               .filter((F.col(\"event_page\") != F.col(\"previous_page\")) | F.col(\"previous_page\").isNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.createOrReplaceTempView(\"filtered_clickstream_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_df = spark.sql(\"\"\"\n",
    "    WITH user_session_routes AS (\n",
    "        SELECT \n",
    "            user_id, \n",
    "            session_id, \n",
    "            COLLECT_LIST(event_page) AS page_sequence\n",
    "        FROM filtered_clickstream_data\n",
    "        GROUP BY user_id, session_id\n",
    "    ),\n",
    "    route_frequency AS (\n",
    "        SELECT \n",
    "            ARRAY_JOIN(page_sequence, '-') AS route, \n",
    "            COUNT(1) AS route_count\n",
    "        FROM user_session_routes\n",
    "        GROUP BY route\n",
    "    )\n",
    "    SELECT route, route_count\n",
    "    FROM route_frequency\n",
    "    ORDER BY route_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|route              |route_count|\n",
      "+-------------------+-----------+\n",
      "|main               |8185       |\n",
      "|main-archive       |1113       |\n",
      "|main-rabota        |1047       |\n",
      "|main-internet      |897        |\n",
      "|main-bonus         |870        |\n",
      "|main-news          |769        |\n",
      "|main-tariffs       |677        |\n",
      "|main-online        |587        |\n",
      "|main-vklad         |517        |\n",
      "|main-rabota-archive|170        |\n",
      "+-------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "routes_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
